{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "# import from keras.preprocessing import sequence\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasource_dir = r'/home/alexsun/ML/data_center/standford_large_movie_review_dataset/aclImdb/'\n",
    "\n",
    "class Mysentences(object):\n",
    "    def __init__(self, dir_name):\n",
    "        self.dir_name = dir_name\n",
    "        \n",
    "    @staticmethod\n",
    "    def multiple_replace(text, adict):\n",
    "        rx = re.compile('|'.join(map(re.escape, adict)))\n",
    "        def one_xlat(match):\n",
    "            return adict[match.group(0)]\n",
    "        return rx.sub(one_xlat, text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def txt_process(txt):\n",
    "        text_replace_dic = {'<br />':'', '\\n':' ', '.':'', '?':'', '*':'', '$':''}\n",
    "        return Mysentences.multiple_replace(txt, text_replace_dic)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for filename in os.listdir(self.dir_name):\n",
    "            with open(os.path.join(self.dir_name, filename), encoding='utf-8') as f:\n",
    "                new_line = Mysentences.txt_process(f.read())\n",
    "                yield new_line.split()\n",
    "                \n",
    "# print([line for line in Mysentences('/home/alexsun/aha')])\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v method begins\n",
      "word2vec training process ignored, file exists... \n",
      "word2idex: the  642\n",
      "total number of words: 36763\n"
     ]
    }
   ],
   "source": [
    "# 利用word2vec训练出来的模型，　返回每个词语的索引，　词向量\n",
    "# 参考：https://github.com/BUPTLdy/Sentiment-Analysis/blob/master/code/Sentiment_lstm.py\n",
    "def create_dics(model):\n",
    "    if (model is not None):\n",
    "        gensim_dict = Dictionary()\n",
    "        gensim_dict.doc2bow(model.wv.vocab.keys(),\n",
    "                          allow_update=True)\n",
    "        w2index = {v:k+1 for k,v in gensim_dict.items()} # 仅仅保留出现次数大于min_count的单词索引\n",
    "        w2vec = model.wv\n",
    "        # 保留这句话，　可以在投喂神经网络计算前加上     \n",
    "        # combined_vec = sequence.pad_sequences(combined, maxlen=maxlen)               \n",
    "    else:\n",
    "        print('No data provided')\n",
    "    return w2index, w2vec   \n",
    "\n",
    "pickle_path = os.path.join(datasource_dir, 'pickles/')\n",
    "\n",
    "def word2vec_train():\n",
    "    print('w2v method begins')\n",
    "    n_iterations = 4\n",
    "    total_examples = 50002\n",
    "    save_pickle_word2vec = pickle_path + 'word2vec.pickle'\n",
    "    if(os.path.exists(save_pickle_word2vec)):\n",
    "        print('word2vec training process ignored, file exists... ')\n",
    "        model = Word2Vec.load(save_pickle_word2vec)\n",
    "    else:\n",
    "        sentences = Mysentences(os.path.join(datasource_dir, 'total/'))\n",
    "        model = Word2Vec(size=100, window=5, min_count=10, workers=4)\n",
    "        model.build_vocab(sentences)\n",
    "        # model.sort_vocab()\n",
    "        print('vocab builded')\n",
    "        # i dont know what's epochs, but it must be passed\n",
    "        model.train(sentences, total_examples=total_examples, epochs=10)\n",
    "        print('model trained!')\n",
    "        model.save(save_pickle_word2vec)\n",
    "        print('word2vec model has been trained! and the model has been saved')\n",
    "    index2word, word_vectors = create_dics(model=model)\n",
    "    return model, index2word, word_vectors\n",
    "    # return model\n",
    "\n",
    "\n",
    "model, word2index, word_vectors = word2vec_train()\n",
    "print('word2idex: the ', word2index['the'])\n",
    "print('total number of words: %d' %(len(word2index.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0174416840107\n",
      "0.626315284298\n",
      "0.665314994582\n",
      "-0.0225523624497\n",
      "0.550402141467\n",
      "0.082536765306\n",
      "0.570134050151\n"
     ]
    }
   ],
   "source": [
    "# simple test block\n",
    "# print(word_vectors['funny'])\n",
    "# print(word_vectors['interesting'])\n",
    "print(model.similarity('bus', 'fun'))\n",
    "print(model.similarity('bus', 'car'))\n",
    "print(model.similarity('China', 'USA'))\n",
    "print(model.similarity('China', 'fun'))\n",
    "print(model.similarity('Queen', 'King'))\n",
    "print(model.similarity('Queen','car'))\n",
    "print(model.similarity('Korea', 'USA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
